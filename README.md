# ü¶ô LoRA Training Application

Applicazione Python modulare per la generazione di dataset e il fine-tuning di modelli di linguaggio utilizzando LoRA (Low-Rank Adaptation).

## üéØ Caratteristiche

- **Generazione Dataset**: Crea dataset di training usando modelli AI con tre personalit√† diverse (Analyst, Creative, Consensus)
- **Training LoRA/QLoRA**: Fine-tuning efficiente di modelli come Gemma 2B usando LoRA
- **Inference**: Interfaccia per testare modelli addestrati, con modalit√† interattiva
- **Architettura Modulare**: Codice pulito e ben organizzato in moduli separati
- **CLI Completa**: Interfaccia a riga di comando per tutte le operazioni

## üìÅ Struttura del Progetto

```
lora-test/
‚îú‚îÄ‚îÄ config.py              # Configurazioni e costanti
‚îú‚îÄ‚îÄ api_client.py          # Client per API OpenAI-compatibili
‚îú‚îÄ‚îÄ dataset_generator.py   # Generazione dataset
‚îú‚îÄ‚îÄ lora_trainer.py        # Training LoRA
‚îú‚îÄ‚îÄ inference.py           # Inference con modelli addestrati
‚îú‚îÄ‚îÄ utils.py               # Funzioni utility
‚îú‚îÄ‚îÄ main.py               # Entry point CLI
‚îú‚îÄ‚îÄ requirements.txt       # Dipendenze Python
‚îî‚îÄ‚îÄ README.md             # Questa documentazione
```

## üöÄ Installazione

### Prerequisiti

- Python 3.9+
- GPU con CUDA (consigliato: 12-16GB VRAM per training, 8-10GB con QLoRA)
- Per l'inference senza GPU, √® possibile usare CPU (pi√π lento)

### Setup

```bash
# Clona o naviga nella directory del progetto
cd lora-test

# Crea un ambiente virtuale (opzionale ma consigliato)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# oppure
venv\Scripts\activate  # Windows

# Installa le dipendenze
pip install -r requirements.txt

# Per CUDA 12.x (se necessario)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

## ‚öôÔ∏è Configurazione

### Variabili d'Ambiente per API Models

Per la generazione del dataset, devi configurare almeno un modello API:

```bash
# Modelli predefiniti
export API_KEY_SCOUT17B='your_key_here'
export API_KEY_GEMINI_FLASH='your_key_here'
export API_KEY_MISTRAL_SMALL='your_key_here'

# Oppure modelli custom
export API_KEY_1='your_openai_key'
export API_URL_1='https://api.openai.com/v1'
export MODEL_ID_1='gpt-4o-mini'
```

### Variabili per Training (opzionali)

```bash
export MODEL_ID='google/gemma-2-2b-it'
export LORA_R=16
export LORA_ALPHA=32
export EPOCHS=2
export BATCH_SIZE=2
export LR=2e-4
```

## üìñ Utilizzo

L'applicazione fornisce una CLI con diversi comandi:

### 1. Test Connettivit√† Modelli

Verifica che i modelli API siano accessibili:

```bash
python main.py test-models
```

Output esempio:
```
üîå Testing model connectivity...

Results:
  llama-4-scout-17b-16e-w4a16: ‚úÖ Connected (response: OK)
  gemini-2.5-flash: ‚úÖ Connected (response: OK)
  Mistral-Small-24B-W8A8: ‚úÖ Connected (response: OK)

3/3 models working
```

### 2. Generare Dataset

Genera un dataset di training con esempi multi-personalit√†:

```bash
# Genera 50 esempi
python main.py generate --batch-size 50

# Con parametri personalizzati
python main.py generate \
    --batch-size 100 \
    --max-tokens 1000 \
    --temperature 0.9 \
    --output-dir ./my_dataset \
    --seed 42 \
    --clean
```

**Opzioni:**
- `--batch-size`: Numero di esempi da generare (default: 10)
- `--max-tokens`: Token massimi per risposta (default: 800)
- `--temperature`: Temperatura di generazione (default: 0.85)
- `--output-dir`: Directory di output (default: auto-generata con timestamp)
- `--seed`: Seed random per riproducibilit√† (default: None)
- `--clean`: Pulisce il dataset dopo la generazione

### 3. Training LoRA

Addestra un modello usando il dataset generato:

```bash
# Training base
python main.py train --dataset ./outputs/20251029-022151/dataset.jsonl

# Training personalizzato
python main.py train \
    --dataset ./my_dataset/dataset.jsonl \
    --model-id google/gemma-2-2b-it \
    --epochs 3 \
    --batch-size 4 \
    --learning-rate 3e-4 \
    --lora-r 32 \
    --lora-alpha 64 \
    --output-dir ./my_lora_model
    
# Merge LoRA con modello base dopo training
python main.py train \
    --dataset ./my_dataset/dataset.jsonl \
    --merge
```

**Opzioni:**
- `--dataset`: Path al file JSONL del dataset (auto-scoperto se omesso)
- `--model-id`: ID del modello base (default: google/gemma-2-2b-it)
- `--epochs`: Numero di epoche (default: 2)
- `--batch-size`: Batch size (default: 2)
- `--learning-rate`: Learning rate (default: 2e-4)
- `--lora-r`: Rank LoRA (default: 16)
- `--lora-alpha`: Alpha LoRA (default: 32)
- `--no-4bit`: Disabilita quantizzazione 4-bit
- `--output-dir`: Directory di output (default: auto-generata)
- `--merge`: Merge LoRA con base model dopo il training
- `--config-from-env`: Carica configurazione da variabili d'ambiente

### 4. Inference

Usa il modello addestrato per generare testo:

#### Modalit√† Prompt Singolo

```bash
python main.py inference \
    --model-id google/gemma-2-2b-it \
    --adapter ./lora_gemma2b_20251029-021642 \
    --prompt "Spiega cosa sono le LoRA in machine learning"
```

#### Modalit√† Interattiva (Chat)

```bash
python main.py inference \
    --model-id google/gemma-2-2b-it \
    --adapter ./lora_gemma2b_20251029-021642 \
    --interactive
```

Comandi interattivi:
- `quit` / `exit` / `q`: Esci dalla chat
- `reset`: Cancella la cronologia conversazione
- `history`: Visualizza la cronologia

#### Con Modello Merged

Se hai fatto il merge del modello:

```bash
python main.py inference \
    --model-id ./my_lora_model_merged \
    --merged \
    --interactive
```

**Opzioni:**
- `--model-id`: ID modello base o path a modello merged (required)
- `--adapter`: Path all'adapter LoRA (opzionale se --merged)
- `--merged`: Il modello √® gi√† merged (non serve adapter)
- `--prompt`: Prompt per inference singola
- `--interactive`: Avvia chat interattiva
- `--system-prompt`: System prompt (default: "You are a helpful assistant.")
- `--max-tokens`: Token massimi da generare (default: 256)
- `--temperature`: Temperatura (default: 0.7)
- `--top-p`: Nucleus sampling (default: 0.9)
- `--no-4bit`: Disabilita quantizzazione 4-bit

### 5. Pulire Dataset

Rimuovi record duplicati o invalidi:

```bash
python main.py clean input.jsonl --output cleaned.jsonl
```

## üîß Uso come Libreria

Puoi anche importare i moduli direttamente nel tuo codice Python:

### Esempio: Generazione Dataset

```python
from config import DatasetConfig, get_default_models
from dataset_generator import DatasetGenerator

# Configura
config = DatasetConfig(batch_size=20, max_tokens=1000)
models = get_default_models()

# Genera dataset
generator = DatasetGenerator(config, models)
generator.run_batch()

# Valida
results = generator.validate_output()
print(results)
```

### Esempio: Training

```python
from config import LoRAConfig
from lora_trainer import LoRATrainer

# Configura
config = LoRAConfig(
    model_id='google/gemma-2-2b-it',
    num_epochs=3,
    lora_r=32
)

# Training
trainer = LoRATrainer(config, dataset_path='./dataset.jsonl')
metrics = trainer.run_full_training()
print(metrics)
```

### Esempio: Inference

```python
from inference import LoRAInference

# Inizializza
engine = LoRAInference(
    base_model_id='google/gemma-2-2b-it',
    adapter_path='./lora_output'
)

# Chat
response = engine.chat_simple(
    user_message="Explain LoRA",
    max_new_tokens=200
)
print(response)
```

## üìä Formati Dataset

Il dataset generato √® in formato JSONL, con una struttura come:

```json
{
  "id": "uuid-here",
  "persona": "analyst",
  "input": [
    {"role": "system", "content": "You are Analyst..."},
    {"role": "user", "content": "Topic: ..."}
  ],
  "output": "Generated response...",
  "meta": {
    "model_name": "gemini-2.5-flash",
    "model_id": "gemini-2.5-flash",
    "topic": "Kubernetes scheduling",
    "seed": 12345,
    "temperature": 0.85,
    "created_utc": "2025-10-29T02:21:51.123456Z"
  }
}
```

## üé≠ Personalit√† (Personae)

Il dataset viene generato usando tre personalit√† diverse:

1. **üß† Analyst**: Preciso, strutturato, con step-by-step e bullet points
2. **üí° Creative**: Divergente, metaforico, con esempi narrativi
3. **ü§ù Consensus**: Bilanciato, sintetizza le due prospettive

Questo crea un dataset pi√π vario e ricco per il fine-tuning.

## üêõ Troubleshooting

### Errore: "No models configured"

Assicurati di aver impostato almeno una API key:
```bash
export API_KEY_GEMINI_FLASH='your_key_here'
```

### Errore CUDA out of memory

- Riduci `--batch-size` (es. 1 invece di 2)
- Usa `--gradient-accumulation-steps` pi√π alto (gi√† default a 8)
- Assicurati che `use_4bit=True` (default)
- Riduci `--max-seq-length`

### Modelli non si caricano

Verifica di avere spazio disco sufficiente (i modelli possono pesare diversi GB).

### Errori di import

Reinstalla le dipendenze:
```bash
pip install --upgrade -r requirements.txt
```

## üìù Note

- **GPU Requirement**: Il training richiede una GPU. Per inference, la CPU funziona ma √® pi√π lenta.
- **Quantizzazione**: QLoRA 4-bit √® abilitata di default per ridurre l'uso di memoria.
- **Dataset Size**: Pi√π esempi generalmente migliorano la qualit√†, ma aumentano il tempo di training.
- **LoRA Rank**: Valori tipici sono 8-64. Pi√π alto = pi√π parametri addestrabili ma pi√π memoria.

## ü§ù Contributi

Questo √® un progetto di esempio/template. Sentiti libero di modificarlo secondo le tue esigenze!

## üìÑ Licenza

Questo codice √® fornito "as-is" per uso educativo e di ricerca.

## üîó Risorse Utili

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [PEFT Documentation](https://huggingface.co/docs/peft)
- [TRL (Transformer Reinforcement Learning)](https://huggingface.co/docs/trl)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

---

**Buon Training! üöÄ**

